{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "SeHupMR7x-q4"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def adaboost_train(X, y, T):\n",
        "    \"\"\"\n",
        "    X: Dane treningowe, macierz NxM, gdzie N to liczba próbek, M to liczba cech.\n",
        "    y: Etykiety klas, wektor o długości N.\n",
        "    T: Liczba iteracji boostingu (liczba słabych klasyfikatorów).\n",
        "\n",
        "    Zwraca:\n",
        "    - klasyfikatory\n",
        "    - ich wagi\n",
        "    \"\"\"\n",
        "    N, M = X.shape\n",
        "    # Inicjalizacja wag dla każdego przykładu (wszystkie wagi początkowo równe)\n",
        "    w = np.ones(N) / N\n",
        "\n",
        "    # Lista słabych klasyfikatorów\n",
        "    classifiers = []\n",
        "    # Lista wag dla każdego klasyfikatora\n",
        "    alphas = []\n",
        "\n",
        "    for t in range(T):\n",
        "        # Tworzenie prostego klasyfikatora - tu użyjemy drzewo pniowe (ang. decision stump)\n",
        "        stump = DecisionTreeClassifier(max_depth=1)\n",
        "\n",
        "        # Trenowanie klasyfikatora na danych z uwzględnieniem wag\n",
        "        stump.fit(X, y, sample_weight=w)\n",
        "        # Predykcje klasyfikatora\n",
        "        y_pred = stump.predict(X)\n",
        "\n",
        "        # Obliczanie błędu klasyfikatora\n",
        "        error = np.sum(w * (y_pred != y)) / np.sum(w)\n",
        "\n",
        "        # Obliczanie wagi klasyfikatora\n",
        "        alpha = 0.5 * np.log((1 - error) / (error + 1e-10))  # Dodajemy małą wartość, by uniknąć dzielenia przez zero\n",
        "\n",
        "        # Aktualizacja wag dla przykładów\n",
        "        w *= np.exp(-alpha * y * y_pred)\n",
        "        w /= np.sum(w)  # Normalizacja wag, aby sumowały się do 1\n",
        "\n",
        "        # Zapis klasyfikatora i jego wagi\n",
        "        classifiers.append(stump)\n",
        "        alphas.append(alpha)\n",
        "\n",
        "    return classifiers, alphas\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def adaboost_predict(X, classifiers, alphas):\n",
        "    \"\"\"\n",
        "    X: Dane testowe\n",
        "    classifiers: lista słabych klasyfikatorów\n",
        "    alphas: lista wag klasyfikatorów\n",
        "\n",
        "    Zwraca przewidywania dla danych X.\n",
        "    \"\"\"\n",
        "    # Inicjalizacja przewidywań jako suma ważona klasyfikatorów\n",
        "    final_predictions = np.zeros(X.shape[0])\n",
        "\n",
        "    for classifier, alpha in zip(classifiers, alphas):\n",
        "        # Zwiększanie wyniku o wagę razy predykcję danego klasyfikatora\n",
        "        final_predictions += alpha * classifier.predict(X)\n",
        "\n",
        "    # Finalna decyzja to znak sumy ważonej\n",
        "    return np.sign(final_predictions)\n"
      ],
      "metadata": {
        "id": "YrUNyLoqyFEY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Załaduj dane Iris\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target"
      ],
      "metadata": {
        "id": "nUz3Anp6yP7m"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Wybierz tylko dwie klasy (dla uproszczenia)\n",
        "X = X[y != 2]\n",
        "y = y[y != 2]\n",
        "\n",
        "# Zmiana etykiet klas na {-1, 1}\n",
        "y = np.where(y == 0, -1, 1)\n",
        "\n",
        "# Podziel dane na zbiór treningowy i testowy\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
      ],
      "metadata": {
        "id": "j7ife6E5ySAx"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Trenuj AdaBoost\n",
        "T = 10  # Liczba słabych klasyfikatorów\n",
        "classifiers, alphas = adaboost_train(X_train, y_train, T)\n",
        "\n",
        "# Przewiduj etykiety na zbiorze testowym\n",
        "y_pred = adaboost_predict(X_test, classifiers, alphas)\n",
        "\n",
        "# Sprawdź dokładność\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Dokładność: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WVpiLwvySh4",
        "outputId": "36861be3-0ebf-42fe-8958-2c84fb27e14f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dokładność: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def plot_decision_boundary(X, y, classifiers, alphas):\n",
        "    # Ustawianie siatki punktów do wizualizacji\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "                         np.arange(y_min, y_max, 0.02))\n",
        "\n",
        "    # Przewidywanie wartości dla każdego punktu siatki\n",
        "    Z = adaboost_predict(np.c_[xx.ravel(), yy.ravel()], classifiers, alphas)\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "      # Rysowanie wykresu\n",
        "    plt.contourf(xx, yy, Z, alpha=0.8 )\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y )\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "QB1zy77lyWlV"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZcfNh_LTysGY"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KaluqC_92e3z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}